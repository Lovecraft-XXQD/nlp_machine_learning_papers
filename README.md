# NLP 和机器学习论文中文翻译

在阅读过程中发现有翻译不好的地方，可以直接修改

有其它想要阅读和翻译的论文，将论文转换成 HTML 后上传，有问题请在 Issues 中留言

LaTex 转 HTML 的方法参见 [Wiki](https://github.com/yiyibooks/nlp_machine_learning_papers/wiki/%E5%A6%82%E4%BD%95%E5%B0%86-LaTex-%E8%BD%AC%E6%8D%A2%E6%88%90-HTML) 

[Big Self-Supervised Models are Strong Semi-Supervised Learners, 2020](https://www.yiyibooks.cn/nlp/SimCLRv2/index.html), 图像领域无监督预训练 + 有监督微调 SimCLR v2

[ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems, 2020](https://www.yiyibooks.cn/nlp/ConvLab2/index.html), 对话系统工具包 ConLab-2

[Poly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring, 2020](https://www.yiyibooks.cn/nlp/poly-encoder/index.html), 多语句评分 poly-encoder

[FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence, 2020](https://www.yiyibooks.cn/nlp/FixMatch/index.html), 半监督学习 FixMatch

[XLNet: Generalized Autoregressive Pretraining for Language Understanding, 2019](https://www.yiyibooks.cn/nlp/XLNet/index.html), xlnet

[Unsupervised Data Augmentation for Consistency Training, 2019](https://www.yiyibooks.cn/nlp/uda/index.html), 无监督数据的增强 UDA

[EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks, 2019](https://yiyibooks.cn/nlp/EDA/index.html), 有监督数据的增强 EDA

[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, 2019](https://www.yiyibooks.cn/nlp/SentenceBERT_Sentence_Embeddings_using_Siamese_BERTNetworks/index.html), sentence-bert

[RoBERTa: A Robustly Optimized BERT Pretraining Approach, 2019](https://www.yiyibooks.cn/nlp/roberta/index.html), 预训练方法优化的 BERT, Roberta

[Albert: A Lite Bert For Self-Supervised Learning Of Language Representations, 2019](https://yiyibooks.cn/yiyibooks/A_LITE_BERT_FOR_SELFSUPERVISED_LEARNING_OF_LANGUAGE_REPRESENTATIONS/index.html), 内存优化的 BERT，Albert

[An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction, 2019](https://yiyibooks.cn/yiyibooks/An_Evaluation_Dataset_for_Intent_Classification/index.html), Intent Classification dataset

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018](https://www.yiyibooks.cn/nlp/bert/main.html), bert

[Attention Is All You Need, 2017](https://yiyibooks.cn/yiyibooks/Attention_Is_All_You_Need/index.html), Transformer

[Layer Normalization, 2016](https://www.yiyibooks.cn/nlp/layer_norm/index.html), 层归一化 Layer Normalization

[Gaussian Error Linear Units (GELUs), 2016](https://www.yiyibooks.cn/nlp/gelu/main.html), 非线性 gelu

[Neural Machine Translation of Rare Words with Subword Units, 2016](https://yiyibooks.cn/yiyibooks/Neural_Machine_Translation_of_Rare_Words_with_Subword_Units/index.html), 子词 subword, BPE

[Pointer Networks, 2015](https://www.yiyibooks.cn/nlp/pointer_network/index.html), 指针网络 pointer networks

[Effective Approaches to Attention-based Neural Machine Translation, 2015](https://yiyibooks.cn/yiyibooks/Effective_Approaches_to_Attention_Based_Neural_Machine_Translation/index.html)

[Distilling the Knowledge in a Neural Network, 2015](https://www.yiyibooks.cn/nlp/Knowledge_Distilling/index.html), 知识蒸馏 Knowledge distilling

[Neural Machine Translation by Jointly Learning to Align and Translate, 2014](https://yiyibooks.cn/yiyibooks/Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate/index.html), Seq2seq with attention

[Convolutional Neural Networks for Sentence Classification, 2014](https://www.yiyibooks.cn/nlp/textcnn/index.html), 文本分类 textcnn

[BLEU: a Method for Automatic Evaluation of Machine Translation, 2002](https://yiyibooks.cn/yiyibooks/BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation/index.html), 机器翻译评价指标 bleu
